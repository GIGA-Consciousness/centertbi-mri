{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTBI Neurobot databases merger and downloader for WP15\n",
    "By Stephen Larroque @ Coma Science Group, GIGA Research, University of Liege\n",
    "Creation date: 2018-10-27\n",
    "License: MIT\n",
    "v0.2.0\n",
    "\n",
    "DESCRIPTION:\n",
    "Generic tool to merge two neurobot CSV databases based on GUPI\n",
    "\n",
    "INSTALL NOTE:\n",
    "You need to pip install pandas before launching this script.\n",
    "Tested on Python 2.7.13\n",
    "\n",
    "USAGE: For the merge: First download from Neurobot the Imaging (and Subject) database in one CSV, then the Outcomes database in another CSV. Then you need to convert the comma separated csv files to semicolon (or change all the pd.read_csv() calls in this notebook to use commas instead of semicolon). Then you can input the path of these csv files below and run all cells!\n",
    "For the imaging nifti download: you need to input your credentials in login.cfg.example and rename to login.cfg. You also need the Imaging/Subject databases in a csv.\n",
    "\n",
    "TODO:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forcefully autoreload all python modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUX FUNCTIONS\n",
    "\n",
    "import os, sys\n",
    "\n",
    "cur_path = os.path.realpath('.')\n",
    "sys.path.append(os.path.join(cur_path, 'csg_fileutil_libs'))  # for unidecode and cleanup_name, because it does not support relative paths (yet?)\n",
    "\n",
    "import ast\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from csg_fileutil_libs.aux_funcs import save_df_as_csv, _tqdm, merge_two_df, df_remap_names, concat_vals, df_concatenate_all_but\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "\n",
    "# First (ID) database to merge (both need to have a column 'name'). The merged 'name' column will use the names from this database.\n",
    "db_imaging = r'neurobot_subjects-imaging_20181027.csv'\n",
    "# Second (reference) database to merge. The names will be added as a new column 'name_altx'.\n",
    "db_outcome = r'neurobot_outcomes_20181027.csv'\n",
    "\n",
    "# Output database with the merge results\n",
    "out_db = r'neurobot_merged_db_20181027.csv'\n",
    "# Output folder to store imaging data\n",
    "out_imdir = r'F:\\neurobot_mri'\n",
    "# Uncompress nifti files on-the-fly?\n",
    "nifti_ungz = True\n",
    "# When downloading the nifti files, pass errors (but will still be printed) - else the program will stop at the first exception\n",
    "pass_errors = True\n",
    "\n",
    "verbose = False\n",
    "\n",
    "# What kind of series (DTI, etc) to include? (look at the Imaging.ScanType column)\n",
    "#series_to_include = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load first database\n",
    "df_im = pd.read_csv(db_imaging, sep=';').dropna(how='all').dropna(how='any', subset=['gupi', 'Imaging.SubjectGroup'])  # drop all rows where name is empty (necessary else this will produce an error, we expect the name to exist)\n",
    "df_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load second database\n",
    "df_oc = pd.read_csv(db_outcome, sep=';').dropna(how='all').dropna(how='any', subset=['gupi'])  # drop all rows where name is empty (necessary else this will produce an error, we expect the name to exist)\n",
    "df_oc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only MR 2 weeks\n",
    "df_im = df_im.loc[(df_im['Imaging.Timepoint'] == 'MR 2 weeks'), :]\n",
    "df_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge both databases if gupi matches (keep only those that are in df_im - thus have an MR 2 weeks!)\n",
    "df_merge, df_final = merge_two_df(df_im, df_oc, col='gupi', mode=0, dist_threshold=0, dist_words_threshold=0, skip_sanity=True, keep_nulls=1, returnmerged=True)\n",
    "df_final.set_index('gupi', inplace=True)\n",
    "df_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_df_as_csv(df_final.reset_index(), out_db, fields_order=list(df_final.columns), csv_order_by='gupi'):\n",
    "    print('Merged database successfully saved in %s!' % out_db)\n",
    "else:\n",
    "    print('ERROR: the merged database could not be saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database imaging downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "import shutil\n",
    "from urlparse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading login infos\n",
    "cfgpath = os.path.join(os.getcwd(), 'login.cfg')\n",
    "with open(cfgpath) as f:\n",
    "    # Strip out comments first\n",
    "    login_infos = f.read()\n",
    "    login_infos = re.sub(r'\\\\\\n', '', login_infos)\n",
    "    login_infos = re.sub(r'//.*\\n', '\\n', login_infos)\n",
    "    # Load as JSON\n",
    "    login_infos = json.loads(login_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load first database\n",
    "df_im = pd.read_csv(db_imaging, sep=';').dropna(how='all').dropna(how='any', subset=['gupi', 'Imaging.SubjectGroup'])  # drop all rows where name is empty (necessary else this will produce an error, we expect the name to exist)\n",
    "df_im.set_index('gupi', inplace=True)\n",
    "df_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only MR 2 weeks\n",
    "df_im = df_im.loc[(df_im['Imaging.Timepoint'] == 'MR 2 weeks'), :]\n",
    "df_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precalculate total number of files to download (to show a progressbar)\n",
    "#total = 0\n",
    "#for idx, row in df_final.iterrows():\n",
    "#    try:\n",
    "#        niftiurls = ast.literal_eval(row['Imaging.NiftiURL'])\n",
    "#        total += len(niftiurls)\n",
    "#    except Exception:\n",
    "#        niftiurls = row['Imaging.NiftiURL']\n",
    "#        total += 1\n",
    "#    try:\n",
    "#        dicomheadersurls = ast.literal_eval(row['Imaging.DicomHeaderURL'])\n",
    "#        total += len(dicomheadersurls)\n",
    "#    except Exception:\n",
    "#        dicomheadersurls = row['Imaging.DicomHeaderURL']\n",
    "#        total +=1\n",
    "#total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = df_im['Imaging.NiftiURL'].dropna().count() + df_im['Imaging.DicomHeaderURL'].dropna().count()\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pathsafe(s):\n",
    "    \"\"\"Make sure a string is path safe (replace any path unsafe character). From https://stackoverflow.com/a/295146/1121352\"\"\"\n",
    "    import string\n",
    "    valid_chars = \"-_.() %s%s\" % (string.ascii_letters, string.digits)\n",
    "    valid_chars = frozenset(valid_chars)\n",
    "    return ''.join(c for c in s if c in valid_chars)\n",
    "\n",
    "# Create output directory if does not exist\n",
    "if not os.path.exists(out_imdir):\n",
    "    os.makedirs(out_imdir)\n",
    "\n",
    "# Prepare progressbar\n",
    "pbar = _tqdm(total=total, desc=\"DOWN\", unit=\"files\")\n",
    "\n",
    "# Iterate for each row/gupi/subject\n",
    "for gupi, row in df_im.iterrows():\n",
    "    niftiurl = row['Imaging.NiftiURL']\n",
    "    dicomheaderurl = row['Imaging.DicomHeaderURL']\n",
    "    if verbose:\n",
    "        print(niftiurl, dicomheaderurl)\n",
    "\n",
    "    for url in (niftiurl, dicomheaderurl):\n",
    "        # Get filename from server\n",
    "        filename = os.path.basename(urlparse(url).path)\n",
    "        # Build local filepath according to site name and scan type (and clean up each item to make sure it's pathsafe)\n",
    "        filepath = os.path.join(out_imdir, pathsafe(row['Subject.SiteCode']), pathsafe(gupi), pathsafe(row['Imaging.ScanType'].replace(' ', '_') + ' ' + row['Imaging.SeriesDescription'].replace(' ', '_')), pathsafe(filename))\n",
    "\n",
    "        # Access online the resource\n",
    "        r = requests.get(url, auth=(login_infos['username'],login_infos['password']))\n",
    "\n",
    "        # Try to download\n",
    "        try:\n",
    "            if r.status_code == 200:\n",
    "                # Create folder if necessary\n",
    "                if not os.path.exists(os.path.dirname(filepath)):\n",
    "                    os.makedirs(os.path.dirname(filepath))\n",
    "                # Write the content (download)\n",
    "                with open(filepath, 'wb') as out:\n",
    "                    for bits in r.iter_content():\n",
    "                        out.write(bits)\n",
    "                # Uncompress file if user wants\n",
    "                if nifti_ungz and filepath[-3:] == '.gz':\n",
    "                    with gzip.open(filepath, 'rb') as f_in:\n",
    "                        with open(filepath[:-3], 'wb') as f_out:\n",
    "                            shutil.copyfileobj(f_in, f_out)\n",
    "                    os.remove(filepath)\n",
    "                pbar.update()\n",
    "            else:\n",
    "                print('Warning: could not download file: %s of subject gupi %s, got this response code: %i' % (filename, row['gupi'], r.status_code))\n",
    "        except Exception as exc:\n",
    "            print(exc)\n",
    "            if not pass_errors:\n",
    "                raise(exc)\n",
    "\n",
    "print('All files successfully downloaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
