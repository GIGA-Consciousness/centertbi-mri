{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Center-TBI statistics (from full dump via XNAT REST API/pyxnat)\n",
    "Compute statistics from the full database dumped in JSON from XNAT REST API (via pyxnat). Please generate beforehand `xnat_data_extrat.json` by executing `xnat_data_extractor.ipynb` (or get the file somewhere else, since it contains private project data it cannot be stored publicly here).\n",
    "\n",
    "Please edit login.cfg with your credentials before executing this script.\n",
    "\n",
    "Before (re-)running this script, please clear output, shutdown and relaunch kernel, close down and reopen your browser, and then (re-)launch all the cells! Else the memory is not correctly freed (this is a bug in ipywidgets or jupyter notebook).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# BEWARE: autoreload works on functions and on general code, but NOT on new class methods:\n",
    "# if you add or change the name of a method, you have to reload the kernel!\n",
    "# also it will fail if you use super() calls in the classes you change\n",
    "\n",
    "# Profilers:\n",
    "# http://pynash.org/2013/03/06/timing-and-profiling/\n",
    "# http://mortada.net/easily-profile-python-code-in-jupyter.html\n",
    "# use %lprun -m module func(*args, **kwargs)\n",
    "try:\n",
    "    %load_ext line_profiler\n",
    "    %load_ext memory_profiler\n",
    "    from fdict import fdict\n",
    "except ImportError as exc:\n",
    "    pass\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Setup some display options for pandas\n",
    "pd.set_option('max_columns', 400)\n",
    "pd.set_option('expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Edit the filepath to the json file here\n",
    "json_filepath = 'xnat_data_extract.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def json_load_dict(json_filepath):\n",
    "    with open(json_filepath, 'rb') as f:\n",
    "        #j = f.read()\n",
    "        jdict = json.load(f)\n",
    "        #del j\n",
    "    return jdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting pertinent database fields into a Dataframe\n",
    "The original structure is very nested, here we extract only the relevant fields to get a 2D matrix of only the relevant fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the json dict\n",
    "jdict = json_load_dict(json_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reorder the json dict so that it is organized per scan session instead of per project\n",
    "def ensure_list(obj):\n",
    "    '''Ensure an object is a collection/list, and not a single object (this simplifies looping)'''\n",
    "    if '@ID' in obj:\n",
    "        # If it has an id attribute, this is a single object, we need to transform it into a list\n",
    "        obj = {obj['@ID']: obj}  # we use dicts here instead of lists (for various technical reasons)\n",
    "    return obj\n",
    "\n",
    "def copy_dict_exclude(d, exclude):\n",
    "    '''Copy a dict but exclude the specified keys'''\n",
    "    return {k:v for k,v in d.items() if k not in exclude}\n",
    "\n",
    "exclude_resources = False  # exclude resources because they use up a lot of columns and are pretty much useless for stats (only useful for download)\n",
    "\n",
    "# Initialization, do not touch\n",
    "scans_excludes = ['@xsi:schemaLocation']\n",
    "if exclude_resources:\n",
    "    scans_excludes.append('resources')\n",
    "\n",
    "# Create a dict per scan\n",
    "jdict2 = {'scans': []}\n",
    "for projectname, project in ensure_list(jdict['projects']).items():\n",
    "    # For each project\n",
    "    for subjectname, subject in ensure_list(project['subjects']).items():\n",
    "        # For each subject\n",
    "        for experimentname, experiment in ensure_list(subject['experiments']).items():\n",
    "            # For each experiment\n",
    "            for scanname, scan in ensure_list(experiment['scans']).items():\n",
    "                # For each scan\n",
    "                # Get the scan infos (excluding the columns/fields we do not want)\n",
    "                scan_temp = copy_dict_exclude(scan, scans_excludes)\n",
    "                if 'resources' in scan_temp:\n",
    "                    # Convert to a list so the list of files does not get flattened as columns\n",
    "                    if 'SNAPSHOTS' in scan_temp['resources'] and 'files' in scan_temp['resources']['SNAPSHOTS']:\n",
    "                        scan_temp['resources']['SNAPSHOTS']['files'] = [scan_temp['resources']['SNAPSHOTS']['files'].copy()]\n",
    "                    if 'files' in scan_temp['resources']:\n",
    "                        scan_temp['resources']['files'] = [scan_temp['resources']['files'].copy()]\n",
    "                # Add the scan infos\n",
    "                jdict2['scans'].append(copy_dict_exclude(scan, scans_excludes))  # add this scan record\n",
    "                currecord = jdict2['scans'][-1]  # keep it at hand so we can add infos of above levels\n",
    "                # Add the experiment infos for this scan (these infos are shared across all scans of this experiment, so we duplicate for each scan)\n",
    "                currecord['experiment'] = copy_dict_exclude(experiment, ['scans', 'xnat:scans', '@xsi:schemaLocation'])\n",
    "                # Same for subject infos\n",
    "                currecord['subject'] = copy_dict_exclude(subject, ['experiments', 'xnat:experiments', '@xsi:schemaLocation'])\n",
    "                # Same for project infos\n",
    "                currecord['project'] = copy_dict_exclude(project, ['subjects', 'xnat:subjects', '@xsi:schemaLocation'])\n",
    "                # Compate agregative score for QA fields\n",
    "                try:\n",
    "                    a = json.loads(currecord['experiment']['xnat:assessors']['xnat:assessor']['ext:results_dict'], strict=False)  # need strict=False because of non-escaped \\n and \\r (line return + carriage return)\n",
    "                    currecord['qa.protocol_check'] = a['protocol_check']\n",
    "                    currecord['qa.head_coverage'] = [[k,v] for k,v in a['head_coverage'].items()]\n",
    "                    currecord['qa.protocol_check_global'] = all(1 if v.lower() == 'pass' else 0 for _, v in a['protocol_check'])\n",
    "                    currecord['qa.head_coverage_global'] = all(1 if v.lower() == 'good' else 0 for v in a['head_coverage'].values())\n",
    "                except Exception:\n",
    "                    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del jdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Postprocessing: we convert nested dicts into json strings\n",
    "#for scanid in xrange(len(jdict2['scans'])):\n",
    "#    for key, value in jdict2['scans'][scanid].items():\n",
    "#        if isinstance(value, dict):\n",
    "#            for subkey, subvalue in jdict2['scans'][scanid][key].items():\n",
    "#                if isinstance(subvalue, dict):\n",
    "#                    jdict2['scans'][scanid][key][subkey] = json.dumps(subvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# You should check these files to see if the structure is correct and has all required fields\n",
    "\n",
    "# Save one record for test\n",
    "with open('center-tbi-statistics-from-full-rest-dump_onerecordtest.json', 'w') as f:\n",
    "    json.dump(currecord, f, indent=4, sort_keys=True)\n",
    "# Save all reordered records\n",
    "with open('center-tbi-statistics-from-full-rest-dump_persession.json', 'w') as f:\n",
    "    json.dump(jdict2, f, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Flatten keys so that pandas can access all nested dicts\n",
    "jdict3 = [fdict.flatkeys(scan, sep='.') for scan in jdict2['scans']]\n",
    "# Save flattened records\n",
    "with open('center-tbi-statistics-from-full-rest-dump_persession_flattened.json', 'w') as f:\n",
    "    json.dump(jdict3, f, indent=4, sort_keys=True)\n",
    "# Free up memory\n",
    "del jdict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert to a pandas dataframe!\n",
    "# TODO: if out of memory error, can try to use json-streamer with a mockup-class to return a dict-like generator to pandas: https://github.com/kashifrazzaqui/json-streamer\n",
    "df = pd.io.json.json_normalize(jdict3)\n",
    "# Free up memory\n",
    "del jdict3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show structure of df\n",
    "print('The dataframe has %s (lines, columns)' % str(df.shape))\n",
    "print('Columns of the dataframe: ')\n",
    "print(df.columns)\n",
    "df  # to pretty print, just put the dataframe name as the last line, without print or anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Count of unique values per columns:')\n",
    "for c in df.columns:\n",
    "    try:\n",
    "        print('* %s: %s' % (c, str(len(df[c].unique()))))\n",
    "    except Exception as exc:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Count per value for all columns:')\n",
    "for c in df.columns:\n",
    "    print \"---- %s ---\" % c\n",
    "    print df[c].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = df.apply(lambda df: (df['project.id'], df['qa.protocol_check_global']), axis=1)\n",
    "print('Number of validated scans per center')\n",
    "a.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Only centers with validated scans:')\n",
    "a.apply(lambda x: x if x[1] == True else None).dropna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Total number of validated scans:')\n",
    "len(a.apply(lambda x: x if x[1] == True else None).dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Only centers with unvalidated scans:')\n",
    "a.apply(lambda x: x if x[1] != True else None).dropna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df[['project.id', 'qa.protocol_check_global']].groupby(['project.id']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = df.apply(lambda df: (df['project.id'], df['qa.head_coverage_global']), axis=1)\n",
    "print('Number of validated head coverage scans per center')\n",
    "a.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phantom_validated_centers = df.where(df['experiment.@visit_id'] == 'Phantom')['project.id'].dropna().unique()\n",
    "print('List of Phantom validated centers:')\n",
    "for center in sorted(phantom_validated_centers):\n",
    "    print('* %s' % center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: filter by resting state fmri\n",
    "# TODO: make figures of all of these infos (number of validated/unvalidated scans per center, number of fmri validated scans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rs_fMRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Scan types:')\n",
    "for t in sorted(df['@type'].unique()):\n",
    "    print('* '+ str(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['@type'].value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract only resting state data\n",
    "df_rest = df.loc[df['@type'] == 'rs_fMRI', :]\n",
    "df_rest[['@UID', 'project.id', 'qa.protocol_check_global']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats figures\n",
    "Generate interesting stats figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_boolean(df, col1, col2, morecols=None, where=None, groupby=None, truename='validated', falsename='not validated', title=None, nostats=False, nograph=False):\n",
    "    '''Plot the col2 according to col1, counting for the number of unique values (true/false or true/nan)'''\n",
    "    # Prepare stuff\n",
    "    if not morecols:\n",
    "        morecols = []\n",
    "    if not groupby:\n",
    "        groupby = []\n",
    "    elif isinstance(groupby, str):\n",
    "        groupby = [groupby]\n",
    "    # Extract both columns\n",
    "    if where is not None:\n",
    "        a = df.loc[where, [col1, col2]+morecols+groupby]\n",
    "    else:\n",
    "        a = df[[col1, col2]+morecols+groupby]\n",
    "    # Group by the first column (we call this column the categories)\n",
    "    groupby.insert(0, col1)\n",
    "    b = a.groupby(groupby)\n",
    "    # Aggregate using the sum (or count of True values) and size (the total number of entries)\n",
    "    c = b.aggregate(['sum', 'size'])\n",
    "    # Replace nan/none values by 0 (when there is absolutely no true value for one category)\n",
    "    c.fillna(0, inplace=True)\n",
    "    # Because of aggregation we get a hierarchical multiindex, we flatten the indices for easier access\n",
    "    c.columns = c.columns.get_level_values(1)\n",
    "    # Compute the number of false values (true values count - total values count)\n",
    "    c['size'] = c['size'] - c['sum']\n",
    "    # Rename the columns\n",
    "    c = c.rename(columns={'sum': truename, 'size': falsename})\n",
    "    if not nostats:\n",
    "        # Print the table\n",
    "        print(c)\n",
    "        # Print stats\n",
    "        print(c.describe())\n",
    "        print('Total:\\n'+str(c.sum()))\n",
    "    if not nograph:\n",
    "        # Plot as stacked bars\n",
    "        c.plot.bar(stacked=True, title=title)\n",
    "        plt.show()\n",
    "        # Plot only validated\n",
    "        c[truename].plot.bar(stacked=True, title=(title+' (validated only)'))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_boolean(df, 'project.id', 'qa.protocol_check_global', title='Protocol check global (all scan types)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_boolean(df, 'project.id', 'qa.head_coverage_global', truename='Good', falsename='Bad', title='Head coverage global (all scan types)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_boolean(df, 'project.id', 'qa.head_coverage_global', where=(df['@type'] == 'rs_fMRI'), title='Protocol check global (rs_fMRI)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Number of subjects with validated rs_fMRI')\n",
    "plot_boolean(df, 'project.id', 'qa.head_coverage_global', where=(df['@type'] == 'rs_fMRI'), groupby=['subject.@ID'], title='Protocol check global (rs_fMRI)', nograph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------\n",
    "## Resources\n",
    "Useful resources:\n",
    "* https://stackoverflow.com/questions/34092808/extract-nested-json-embedded-as-string-in-pandas-dataframe#\n",
    "* https://stackoverflow.com/questions/39899005/how-to-flatten-a-pandas-dataframe-with-some-columns-as-json\n",
    "* http://mindtrove.info/flatten-nested-json-with-pandas/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------\n",
    "## Random code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten_json(y):\n",
    "    # From https://medium.com/@amirziai/flattening-json-objects-in-python-f5343c794b10\n",
    "    out = {}\n",
    "\n",
    "    def flatten(x, name=''):\n",
    "        if type(x) is dict:\n",
    "            for a in x:\n",
    "                flatten(x[a], name + a + '_')\n",
    "        elif type(x) is list:\n",
    "            i = 0\n",
    "            for a in x:\n",
    "                flatten(a, name + str(i) + '_')\n",
    "                i += 1\n",
    "        else:\n",
    "            out[name[:-1]] = x\n",
    "\n",
    "    flatten(y)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def JSON2DICTParser(data):\n",
    "    # From https://stackoverflow.com/questions/20680272/parsing-a-json-string-which-was-loaded-from-a-csv-using-pandas\n",
    "    if data and re.sub('[\\[\\]{}\\s]+', '', data):\n",
    "        try:\n",
    "            j1 = json.loads(data)\n",
    "        except ValueError:\n",
    "            print(data)\n",
    "            raise\n",
    "        return j1\n",
    "    else:\n",
    "        return float('nan')\n",
    "\n",
    "# Load the csv into a Pandas DataFrame\n",
    "#df = pd.read_csv(json_filepath, converters={'software_metadata':JSON2DICTParser, 'parameters_dict':JSON2DICTParser, 'results_dict':JSON2DICTParser})\n",
    "df = pd.read_csv(json_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
